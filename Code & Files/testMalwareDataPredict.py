import pickle
import numpy as np
import random
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_absolute_error


def testMalwareDataPredict(test_type):
    with open("saveModel.pkl", "rb") as f:
        model, select, data_train, data_test, value_train,  value_test, mean_features = pickle.load(f)
    with open("otherModel.pkl", "rb") as f:
        model2, model3, model4 = pickle.load(f)

    # print(legit_test.shape)

    if test_type == "Full":
        label_prediction = model.predict(data_test)
        # for value in label_prediction:
        #     print("Prediction: ", value)
        #     if value == 1:
        #         print("Not Malware...!!!")
        #     else:
        #         print("Malware...!!!")
        train_size= len(value_train)
        print("Trained Data size: ", train_size)
        train_benign = np.count_nonzero(value_train==1)
        print("Training Benign File: ", train_benign)
        train_malware = np.count_nonzero(value_train==0)
        print("Training Malware File: ", train_malware)
        test_size = len(label_prediction)
        print("Test Data Size: ", test_size)
        test_benign = np.count_nonzero(value_test==1)
        print("Test Benign File: ", test_benign)
        test_malware = np.count_nonzero(value_test==0)
        print("Test Malware File: ", test_malware)

        conf_mat = confusion_matrix(value_test, label_prediction)
        true_positives = conf_mat[0][0]
        false_positives = conf_mat[0][1]
        false_negatives = conf_mat[1][0]
        true_negatives = conf_mat[1][1]

        correct_prediction = true_positives + true_negatives
        wrong_prediction = false_positives + false_negatives

        precision = true_positives / (true_positives + false_positives) * 100
        recall = true_positives / (true_positives + false_negatives) * 100
        f1_score = 2 * ((precision * recall) / (precision + recall))

        testing_accuracy = model.score(data_test, value_test) * 100
        training_accuracy = model.score(data_train, value_train) * 100
        print("============RandomForestClassifier===============")
        print("Correct Prediction: ", correct_prediction)
        print("Wrong Prediction: ", wrong_prediction)
        print("Training Accuracy: ", training_accuracy, "%")
        print("Testing Accuracy: ", testing_accuracy, "%")
        print("Precision: ", precision)
        print("Recall: ", recall)
        print("F1 Score: ", f1_score)
        print("False Negatives: ", false_negatives / sum(conf_mat[0]) * 100, "%")
        print("False Positives: ", false_positives / sum(conf_mat[1]) * 100, "%")



        label_prediction = model2.predict(data_test)
        conf_mat = confusion_matrix(value_test, label_prediction)
        true_positives = conf_mat[0][0]
        false_positives = conf_mat[0][1]
        false_negatives = conf_mat[1][0]
        true_negatives = conf_mat[1][1]

        correct_prediction = true_positives + true_negatives
        wrong_prediction = false_positives + false_negatives

        precision = true_positives / (true_positives + false_positives) * 100
        recall = true_positives / (true_positives + false_negatives) * 100
        f1_score = 2 * ((precision * recall) / (precision + recall))

        testing_accuracy = model2.score(data_test, value_test) * 100
        training_accuracy = model2.score(data_train, value_train) * 100
        print("============GradientBoostingClassifier===============")
        print("Correct Prediction: ", correct_prediction)
        print("Wrong Prediction: ", wrong_prediction)
        print("Training Accuracy: ", training_accuracy, "%")
        print("Testing Accuracy: ", testing_accuracy, "%")
        print("Precision: ", precision)
        print("Recall: ", recall)
        print("F1 Score: ", f1_score)
        print("False Negatives: ", false_negatives / sum(conf_mat[0]) * 100, "%")
        print("False Positives: ", false_positives / sum(conf_mat[1]) * 100, "%")




        label_prediction = model3.predict(data_test)
        conf_mat = confusion_matrix(value_test, label_prediction)
        true_positives = conf_mat[0][0]
        false_positives = conf_mat[0][1]
        false_negatives = conf_mat[1][0]
        true_negatives = conf_mat[1][1]

        correct_prediction = true_positives + true_negatives
        wrong_prediction = false_positives + false_negatives

        precision = true_positives / (true_positives + false_positives) * 100
        recall = true_positives / (true_positives + false_negatives) * 100
        f1_score = 2 * ((precision * recall) / (precision + recall))

        testing_accuracy = model3.score(data_test, value_test) * 100
        training_accuracy = model3.score(data_train, value_train) * 100
        print("============DecisionTreeClassifier===============")
        print("Correct Prediction: ", correct_prediction)
        print("Wrong Prediction: ", wrong_prediction)
        print("Training Accuracy: ", training_accuracy, "%")
        print("Testing Accuracy: ", testing_accuracy, "%")
        print("Precision: ", precision)
        print("Recall: ", recall)
        print("F1 Score: ", f1_score)
        print("False Negatives: ", false_negatives / sum(conf_mat[0]) * 100, "%")
        print("False Positives: ", false_positives / sum(conf_mat[1]) * 100, "%")


        """
        label_prediction = model4.predict(data_test)
        conf_mat = confusion_matrix(value_test, label_prediction)
        true_positives = conf_mat[0][0]
        false_positives = conf_mat[0][1]
        false_negatives = conf_mat[1][0]
        true_negatives = conf_mat[1][1]

        correct_prediction = true_positives + true_negatives
        wrong_prediction = false_positives + false_negatives

        precision = true_positives / (true_positives + false_positives) * 100
        recall = true_positives / (true_positives + false_negatives) * 100
        f1_score = 2 * ((precision * recall) / (precision + recall))

        testing_accuracy = model4.score(data_test, value_test) * 100
        training_accuracy = model4.score(data_train, value_train) * 100
        print("============LinearRegression===============")
        print("Correct Prediction: ", correct_prediction)
        print("Wrong Prediction: ", wrong_prediction)
        print("Training Accuracy: ", training_accuracy, "%")
        print("Testing Accuracy: ", testing_accuracy, "%")
        print("Precision: ", precision)
        print("Recall: ", recall)
        print("F1 Score: ", f1_score)
        print("False Negatives: ", false_negatives / sum(conf_mat[0]) * 100, "%")
        print("False Positives: ", false_positives / sum(conf_mat[1]) * 100, "%")
        print("Total Benign Files: ", true_positives + false_negatives)
        print("Total Malware Files: ", true_negatives + false_positives)
        """
        """
        print("Testing Accuracy\n")
        print("RandomForestClassifier: ", model.score(data_test, value_test)*100)
        print("GradientBoostingClassifier: ", model2.score(data_test, value_test)*100)
        print("DecisionTreeClassifier: ", model3.score(data_test, value_test)*100)
        print("LinearRegression: ", model4.score(data_test, value_test)*100)
        """

        # print(type(mal_test))
        # print(type(legit_test))
        # for i, j in zip(label_prediction, mal_test):
        #     # print(i, j)
        #     if i != j:
        #         print("Didn't match...!!!", i, j)

        # label_mae = mean_absolute_error(mal_test, label_prediction)
        # print("The mae value: ", label_mae*100)

    elif test_type == "Single":
        test_no = random.randrange(len(data_test))
        print("Data No: ", test_no)
        test_case = data_test[test_no].reshape(1, -1)

        predict = model.predict(test_case)
        ans = value_test.iloc[test_no]

        if predict == 1:
            print("Not Malware...!!!")
            if predict == ans:
                print("Accurate...!!!")
            else:
                print("Wrong...!!!")
        else:
            print("Malware...!!!")
            if predict == ans:
                print("Accurate...!!!")
            else:
                print("Wrong...!!!")


test_type = "Full"
testMalwareDataPredict(test_type)
